{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "158c6ffd",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41eee75",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Import and get a Spark session and the paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd2363fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{helper.py:36} INFO - \n",
      "Initialized logger\n"
     ]
    }
   ],
   "source": [
    "import etl.helper as helper\n",
    "import etl.process_inbound as process_inbound\n",
    "import etl.process_outbound as process_outbound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21a689a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{helper.py:95} INFO - Initialized Spark instance\n",
      "[{helper.py:63} INFO - Set input_data_path to s3a://4a4e3668e3/data_collection/\n",
      "[{helper.py:67} INFO - Set output_data_path to s3a://4a4e3668e3/data_mart/\n"
     ]
    }
   ],
   "source": [
    "spark, input_path, output_path = helper.get_setup(do_test_s3_access=False)\n",
    "work_mode = helper.get_config_or_default(\"General\", \"work_mode\").split(\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8547efd4",
   "metadata": {},
   "source": [
    "## Test S3 access via Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "346d095c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{helper.py:160} INFO - Testing access to S3 location s3a://4a4e3668e3/data_mart//test.parquet\n",
      "+----+----+\n",
      "|col1|col2|\n",
      "+----+----+\n",
      "|   1|   1|\n",
      "+----+----+\n",
      "\n",
      "[{helper.py:174} INFO - Successfully read from and wrote to S3 location s3a://4a4e3668e3/data_mart//test.parquet\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "helper.test_s3_access(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4cbccc8",
   "metadata": {},
   "source": [
    "# ETL Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aaecbdf",
   "metadata": {},
   "source": [
    "## LOAD source data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "761c8e3e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{process_inbound.py:25} INFO - Found object s3a://4a4e3668e3/data_collection/immo_data.csv: <File-like object S3FileSystem, 4a4e3668e3/data_collection/immo_data.csv>\n",
      "[{process_inbound.py:28} INFO - Reading s3a://4a4e3668e3/data_collection/immo_data.csv into Pandas Dataframe ...\n",
      "[{process_inbound.py:34} INFO - Successfully read s3a://4a4e3668e3/data_collection/immo_data.csv into Pandas Dataframe\n",
      "[{helper.py:368} INFO - Repartitioning dataframe that has 200 partition(s)\n",
      "[{helper.py:370} INFO - Successfully repartitioned dataframe to 1 partition\n",
      "[{helper.py:371} INFO - Writing table_rental_location to s3a://4a4e3668e3/data_mart/table_rental_location.parquet ...\n",
      "[{helper.py:380} INFO - Successfully wrote table_rental_location to s3a://4a4e3668e3/data_mart/table_rental_location.parquet\n",
      "[{process_inbound.py:101} INFO - Extracted parts of Rental data to table table_rental_location\n",
      "[{helper.py:368} INFO - Repartitioning dataframe that has 200 partition(s)\n",
      "[{helper.py:370} INFO - Successfully repartitioned dataframe to 1 partition\n",
      "[{helper.py:371} INFO - Writing table_rental_price_and_cost to s3a://4a4e3668e3/data_mart/table_rental_price_and_cost.parquet ...\n",
      "[{helper.py:380} INFO - Successfully wrote table_rental_price_and_cost to s3a://4a4e3668e3/data_mart/table_rental_price_and_cost.parquet\n",
      "[{process_inbound.py:110} INFO - Extracted parts of Rental data to table table_rental_price_and_cost\n",
      "[{helper.py:368} INFO - Repartitioning dataframe that has 200 partition(s)\n",
      "[{helper.py:370} INFO - Successfully repartitioned dataframe to 1 partition\n",
      "[{helper.py:371} INFO - Writing table_rental_feature to s3a://4a4e3668e3/data_mart/table_rental_feature.parquet ...\n",
      "[{helper.py:380} INFO - Successfully wrote table_rental_feature to s3a://4a4e3668e3/data_mart/table_rental_feature.parquet\n",
      "[{process_inbound.py:121} INFO - Extracted parts of Rental data to table table_rental_feature\n"
     ]
    }
   ],
   "source": [
    "# GET Rental data\n",
    "if \"process_immoscout_data\" in work_mode:\n",
    "    process_inbound.process_immoscout_data(spark, input_path, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5bdacdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{process_inbound.py:139} INFO - Found object s3a://4a4e3668e3/data_collection/zHV_aktuell_csv.2021-09-17.csv\n",
      "[{process_inbound.py:143} INFO - Reading s3a://4a4e3668e3/data_collection/zHV_aktuell_csv.2021-09-17.csv into Spark Dataframe ...\n",
      "[{process_inbound.py:157} INFO - Successfully read s3a://4a4e3668e3/data_collection/zHV_aktuell_csv.2021-09-17.csv into Spark Dataframe\n",
      "[{process_inbound.py:169} INFO - Imported Station data\n",
      "[{helper.py:368} INFO - Repartitioning dataframe that has 200 partition(s)\n",
      "[{helper.py:370} INFO - Successfully repartitioned dataframe to 1 partition\n",
      "[{helper.py:371} INFO - Writing table_stations to s3a://4a4e3668e3/data_mart/table_stations.parquet ...\n",
      "[{helper.py:380} INFO - Successfully wrote table_stations to s3a://4a4e3668e3/data_mart/table_stations.parquet\n"
     ]
    }
   ],
   "source": [
    "# GET Station data\n",
    "if \"process_station_data\" in work_mode:\n",
    "    process_inbound.process_station_data(spark, input_path, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee161688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Datentr„ger in Laufwerk C: ist Windows\n",
      " Volumeseriennummer: 3C16-2530\n",
      "\n",
      " Verzeichnis von C:\\Python\\_Working\\DatEng_Capstone\n",
      "\n",
      "14.01.2022  17:18    <DIR>          .\n",
      "14.01.2022  17:18    <DIR>          ..\n",
      "15.10.2021  21:18               134 .gitignore\n",
      "14.01.2022  17:10    <DIR>          .idea\n",
      "31.10.2021  09:29    <DIR>          .ipynb_checkpoints\n",
      "18.12.2021  00:31    <DIR>          data\n",
      "18.12.2021  10:31               806 dl.cfg\n",
      "07.11.2021  15:42             1.490 dl_example.cfg\n",
      "19.12.2021  15:15             3.490 elt.py\n",
      "06.01.2022  20:22               153 emr_bootstrap.sh\n",
      "20.12.2021  22:32    <DIR>          etl\n",
      "14.01.2022  17:18            13.022 etl_script.ipynb\n",
      "14.01.2022  17:16       219.630.573 event_and_error.log\n",
      "29.10.2021  08:33               265 img_1.png\n",
      "20.10.2021  21:54                 0 immo_data.csv\n",
      "28.10.2021  13:17               830 kaggle_dl.py\n",
      "26.10.2021  17:32    <DIR>          misc\n",
      "12.01.2022  05:52    <DIR>          photon\n",
      "05.01.2022  23:56            20.857 README.md\n",
      "19.12.2021  22:42               227 requirements.txt\n",
      "19.11.2021  01:18    <DIR>          SparkTemp\n",
      "12.10.2021  16:51    <DIR>          venv\n",
      "31.10.2021  08:32    <DIR>          venv2\n",
      "14.01.2022  17:07    <DIR>          _old\n",
      "14.10.2021  10:02    <DIR>          __pycache__\n",
      "              12 Datei(en),    219.671.847 Bytes\n",
      "              13 Verzeichnis(se), 55.610.552.320 Bytes frei\n"
     ]
    }
   ],
   "source": [
    "! dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8202835",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cmd\n",
    "cd photon\n",
    "java -jar ./photon-0.3.5.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9b57628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{helper.py:368} INFO - Repartitioning dataframe that has 200 partition(s)\n",
      "[{helper.py:370} INFO - Successfully repartitioned dataframe to 1 partition\n",
      "[{helper.py:371} INFO - Writing table_mapping_municipal_to_zip to s3a://4a4e3668e3/data_mart/table_mapping_municipal_to_zip.parquet ...\n",
      "[{helper.py:380} INFO - Successfully wrote table_mapping_municipal_to_zip to s3a://4a4e3668e3/data_mart/table_mapping_municipal_to_zip.parquet\n",
      "[{helper.py:368} INFO - Repartitioning dataframe that has 200 partition(s)\n",
      "[{helper.py:370} INFO - Successfully repartitioned dataframe to 1 partition\n",
      "[{helper.py:371} INFO - Writing table_mapping_zip_to_coor to s3a://4a4e3668e3/data_mart/table_mapping_zip_to_coor.parquet ...\n",
      "[{helper.py:380} INFO - Successfully wrote table_mapping_zip_to_coor to s3a://4a4e3668e3/data_mart/table_mapping_zip_to_coor.parquet\n"
     ]
    }
   ],
   "source": [
    "# GET mappings municipal code, zip code\n",
    "if \"process_mappings\" in work_mode:\n",
    "    process_inbound.process_mappings(spark, input_path, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "637379c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{3908912272.py:1} INFO - Preprocessed data sources and saved in data lake\n"
     ]
    }
   ],
   "source": [
    "helper.logger.info(\"Preprocessed data sources and saved in data lake\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70677942",
   "metadata": {},
   "source": [
    "## TRANSFORM data\n",
    "\n",
    "The following two steps utilize the photon geocoding service, which responses to all of the hundreds of thousand address and coordinate lookups. See the Python Jupyter prompt for logged queried. Please stay patient (around four hours), this project version does not incorporate distributed queries.\n",
    "\n",
    "To findest the nearest stations, one query type collects all address elements of an apartment and finds the corresponding coordinates (latitude and longitude). The other query enhances the station data set with zip codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e3c0a21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{process_outbound.py:26} INFO - Preparing table rental location for geocoding...\n",
      "[{helper.py:413} INFO - Successfully imported table_rental_location.parquet to Spark dataframe. Sourced object imported.\n",
      "+---------+-----+-------------------+--------------------+--------------------+-------------------+--------------------+-------+--------------------+--------------------+-----------+\n",
      "|  scoutId| date|             regio1|              regio2|              regio3|            geo_bln|             geo_krs|geo_plz|              street|         streetPlain|houseNumber|\n",
      "+---------+-----+-------------------+--------------------+--------------------+-------------------+--------------------+-------+--------------------+--------------------+-----------+\n",
      "|114751222|Feb20|             Bremen|              Bremen|   Neu_Schwachhausen|             Bremen|              Bremen|  28213|Hermann-Henrich-M...|Hermann-Henrich-M...|         10|\n",
      "|107330219|Sep18|      Niedersachsen|    Holzminden_Kreis|          Holzminden|      Niedersachsen|    Holzminden_Kreis|  37603|           Rauchstr.|           Rauchstr.|         17|\n",
      "|111109355|May19|Nordrhein_Westfalen| Mülheim_an_der_Ruhr|              Broich|Nordrhein_Westfalen| Mülheim_an_der_Ruhr|  45479|B&uuml;lowstra&sz...|         Bülowstraße|        105|\n",
      "|113801700|Oct19|Nordrhein_Westfalen|Recklinghausen_Kreis|            Gladbeck|Nordrhein_Westfalen|Recklinghausen_Kreis|  45968|            Landstr.|            Landstr.|         00|\n",
      "| 61816048|Feb20|      Niedersachsen|      Northeim_Kreis|             Einbeck|      Niedersachsen|      Northeim_Kreis|  37574|Sch&uuml;tzenstra...|      Schützenstraße|        24d|\n",
      "|111314873|May19|Nordrhein_Westfalen|               Essen|             Dellwig|Nordrhein_Westfalen|               Essen|  45356|            Regenweg|            Regenweg|          8|\n",
      "| 90517211|May19|             Bayern|Straubing_Bogen_K...|             Konzell|             Bayern|Straubing_Bogen_K...|  94357|                Haid|                Haid|          2|\n",
      "| 97826343|Feb20|          Thüringen|    Nordhausen_Kreis|          Nordhausen|          Thüringen|    Nordhausen_Kreis|  99734|        Grimmelallee|        Grimmelallee|         33|\n",
      "|102941612|Feb20|            Sachsen|            Chemnitz|           Altendorf|            Sachsen|            Chemnitz|  09116|   Bodelschwinghstr.|  Bodelschwinghstr._|         28|\n",
      "|102169666|Oct19|Nordrhein_Westfalen|               Herne|               Wanne|Nordrhein_Westfalen|               Herne|  44649| Hermannstra&szlig;e|       Hermannstraße|         16|\n",
      "| 95944005|Oct19|Nordrhein_Westfalen|     Heinsberg_Kreis|          Wassenberg|Nordrhein_Westfalen|     Heinsberg_Kreis|  41849|  An der Vogelstange|  An_der_Vogelstange|          5|\n",
      "| 86039254|Sep18|            Sachsen|     Erzgebirgskreis|      Burkhardtsdorf|            Sachsen|     Erzgebirgskreis|  09235|        Lerchensteig|        Lerchensteig|          4|\n",
      "|111286336|May19|             Bayern|             München|      Schwabing_West|             Bayern|             München|  80796|Leonhard-Frank-St...|Leonhard-Frank-St...|          7|\n",
      "|107312815|Sep18|Nordrhein_Westfalen|           Wuppertal|              Barmen|Nordrhein_Westfalen|           Wuppertal|  42281|          Waisenstr.|          Waisenstr.|         63|\n",
      "|115403320|Feb20|     Sachsen_Anhalt|           Magdeburg|        Fermersleben|     Sachsen_Anhalt|           Magdeburg|  39122|         Herbartstr.|         Herbartstr.|         16|\n",
      "|107883690|Feb20|             Hessen|Marburg_Biedenkop...|       Wetter_Hessen|             Hessen|Marburg_Biedenkop...|  35083|        Kandelsgasse|        Kandelsgasse|          9|\n",
      "|113667812|Oct19|             Bayern|             München|         Maxvorstadt|             Bayern|             München|  80333|  Barer Stra&szlig;e|        Barer_Straße|         34|\n",
      "| 62447214|Feb20|            Sachsen|            Chemnitz|             Kaßberg|            Sachsen|            Chemnitz|  09112|         Kanzlerstr.|        Kanzlerstr._|         74|\n",
      "|106150563|Sep18|  Baden_Württemberg|           Heilbronn|Heilbronner_Kerns...|  Baden_Württemberg|           Heilbronn|  74076|            Kalistr.|            Kalistr.|   Kalistr.|\n",
      "|107310836|Sep18|             Bayern|            Erlangen|  Erlangen___Zentrum|             Bayern|            Erlangen|  91054|Marquardsenstra&s...|   Marquardsenstraße|         21|\n",
      "+---------+-----+-------------------+--------------------+--------------------+-------------------+--------------------+-------+--------------------+--------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "[{helper.py:132} INFO - Querying Photon geocode service...\n",
      "[{helper.py:431} INFO - Queried Brandenburger Tor, Pariser Platz, 10117 Berlin.\n",
      "[{helper.py:138} INFO - Successfully queried Photon geocode service. Brandenburger Tor in Berlin is at (13.377701882994323, 52.51628045).\n",
      "[{helper.py:368} INFO - Repartitioning dataframe that has 200 partition(s)\n",
      "[{helper.py:370} INFO - Successfully repartitioned dataframe to 1 partition\n",
      "[{helper.py:371} INFO - Writing table_rental_location_partA to s3a://4a4e3668e3/data_mart/table_rental_location_partA.parquet ...\n",
      "[{helper.py:380} INFO - Successfully wrote table_rental_location_partA to s3a://4a4e3668e3/data_mart/table_rental_location_partA.parquet\n",
      "[{process_outbound.py:117} INFO - Duration Querying PartA: 6879.437386512756\n",
      "[{helper.py:368} INFO - Repartitioning dataframe that has 200 partition(s)\n",
      "[{helper.py:370} INFO - Successfully repartitioned dataframe to 1 partition\n",
      "[{helper.py:371} INFO - Writing table_rental_location_partB to s3a://4a4e3668e3/data_mart/table_rental_location_partB.parquet ...\n",
      "[{helper.py:380} INFO - Successfully wrote table_rental_location_partB to s3a://4a4e3668e3/data_mart/table_rental_location_partB.parquet\n",
      "[{process_outbound.py:132} INFO - Duration Querying PartB: 6772.130926132202\n"
     ]
    }
   ],
   "source": [
    "# GET coordinates of apartments\n",
    "if \"query_coordinates\" in work_mode:\n",
    "\n",
    "    # TEST photon geocoding service\n",
    "    url = \"http://localhost:2322/api\"           # TODO: get url from config-file\n",
    "                                                # TODO: function: instantiate Photon via subprocess\n",
    "\n",
    "    # GET coordinates of rental offers\n",
    "    process_outbound.query_coordinates(spark, input_path=output_path, output_path=output_path, url=url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae73ae36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{helper.py:413} INFO - Successfully imported table_stations.parquet to Spark dataframe. Sourced object imported.\n",
      "[{helper.py:413} INFO - Successfully imported table_mapping_municipal_to_zip.parquet to Spark dataframe. Sourced object imported.\n",
      "[{helper.py:368} INFO - Repartitioning dataframe that has 200 partition(s)\n",
      "[{helper.py:370} INFO - Successfully repartitioned dataframe to 1 partition\n",
      "[{helper.py:371} INFO - Writing table_stations_with_zip to s3a://4a4e3668e3/data_mart/table_stations_with_zip.parquet ...\n",
      "[{helper.py:380} INFO - Successfully wrote table_stations_with_zip to s3a://4a4e3668e3/data_mart/table_stations_with_zip.parquet\n",
      "[{helper.py:413} INFO - Successfully imported table_stations_with_zip.parquet to Spark dataframe. Sourced object imported.\n",
      "[{helper.py:132} INFO - Querying Photon geocode service...\n",
      "[{helper.py:431} INFO - Queried Brandenburger Tor, Pariser Platz, 10117 Berlin.\n",
      "[{helper.py:138} INFO - Successfully queried Photon geocode service. Brandenburger Tor in Berlin is at (13.377701882994323, 52.51628045).\n",
      "[{helper.py:413} INFO - Successfully imported temp_filled_zipcodes.parquet to Spark dataframe. Sourced object imported.\n",
      "[{helper.py:413} INFO - Successfully imported temp_filled_zipcodes2.parquet to Spark dataframe. Sourced object imported.\n",
      "[{helper.py:413} INFO - Successfully imported temp_filled_zipcodes3.parquet to Spark dataframe. Sourced object imported.\n",
      "[{helper.py:413} INFO - Successfully imported temp_filled_zipcodes4.parquet to Spark dataframe. Sourced object imported.\n",
      "[{helper.py:413} INFO - Successfully imported temp_filled_zipcodes3.parquet to Spark dataframe. Sourced object imported.\n",
      "[{helper.py:413} INFO - Successfully imported temp_filled_zipcodes2.parquet to Spark dataframe. Sourced object imported.\n",
      "[{helper.py:413} INFO - Successfully imported temp_filled_zipcodes.parquet to Spark dataframe. Sourced object imported.\n",
      "+------+---------+---------+-----+\n",
      "| SeqNo| Latitude|Longitude|Q_PLZ|\n",
      "+------+---------+---------+-----+\n",
      "|123867| 47.80701|12.352159|83233|\n",
      "|349285|  52.2029| 8.328654|49324|\n",
      "|472829|51.518745| 6.333563|47608|\n",
      "|482614|51.253426|  6.33129|41751|\n",
      "|521292| 50.99461| 7.121529|51469|\n",
      "|650426|52.451042| 9.624685|30827|\n",
      "|669389| 49.63695| 8.645054|64646|\n",
      "|721043|50.554028|13.145452|09477|\n",
      "|736874|51.033203|13.895159|01328|\n",
      "|773499|51.260162| 9.318619|34270|\n",
      "| 17770| 48.92932| 9.562629|71566|\n",
      "| 30123|  49.0733|10.207934|74579|\n",
      "|103990|48.109734|11.830004|85604|\n",
      "|118445|47.979164|11.454481|82067|\n",
      "|182233|50.063656|11.597152|95502|\n",
      "|194137|49.042835|10.359659|91634|\n",
      "|222046| 48.42498|10.878625|86368|\n",
      "|250313|52.726406|13.439049|16348|\n",
      "|265254|52.842613|12.655412|16845|\n",
      "|277554|54.779896| 9.485072|24943|\n",
      "+------+---------+---------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "+------+--------+---------+-----+\n",
      "| SeqNo|Latitude|Longitude|Q_PLZ|\n",
      "+------+--------+---------+-----+\n",
      "|123867|47.80701|12.352159|83233|\n",
      "+------+--------+---------+-----+\n",
      "only showing top 1 row\n",
      "\n",
      "+-----+----+-----------------+---------------+--------------------+---------+---------+----------------+------------+--------------------+--------+----+-----+\n",
      "|SeqNo|Type|             DHID|         Parent|                Name| Latitude|Longitude|MunicipalityCode|Municipality|MunicipalityCode_ZIP|name_ZIP| PLZ|Q_PLZ|\n",
      "+-----+----+-----------------+---------------+--------------------+---------+---------+----------------+------------+--------------------+--------+----+-----+\n",
      "|  138|   A|  de:07334:1747:2|  de:07334:1747|              >Wörth|49.208244| 8.373125|        00000000|           -|                null|    null|null|76726|\n",
      "|  370|   A| de:07334:32490:2| de:07334:32490|                 Bus| 49.07942| 8.198232|        00000000|           -|                null|    null|null|76870|\n",
      "|  971|   A|  de:08111:136:90|   de:08111:136|   Zug. Kirchh. Str.|48.738213| 9.233127|        00000000|           -|                null|    null|null|70619|\n",
      "| 1016|   A|  de:08111:145:80|   de:08111:145|   Bahnüb. Giebelstr|48.802643| 9.092442|        00000000|           -|                null|    null|null|70499|\n",
      "| 1042|   A|   de:08111:147:2|   de:08111:147|                NBus| 48.80714| 9.105045|        00000000|           -|                null|    null|null|70499|\n",
      "| 1143|   A|   de:08111:154:2|   de:08111:154|    St Pfostenwäldle|48.810143| 9.149934|        00000000|           -|                null|    null|null|70469|\n",
      "| 1212|   A|  de:08111:163:90|   de:08111:163|Zug. Neue Weinsteige|48.754753|  9.17435|        00000000|           -|                null|    null|null|70180|\n",
      "| 1278|   A|  de:08111:2053:1|  de:08111:2053|                   1|48.771847| 9.162825|        00000000|           -|                null|    null|null|70178|\n",
      "| 1690|   Q|de:08111:2380:0:4|  de:08111:2380|                   4| 48.81881| 9.242263|        00000000|           -|                null|    null|null|70374|\n",
      "| 2000|   Q|de:08111:2506:0:3|  de:08111:2506|                   3| 48.79206| 9.260705|        00000000|           -|                null|    null|null|70327|\n",
      "| 2264|   Q|de:08111:2591:0:3|  de:08111:2591|Lauchhau Endhalte...|48.738968| 9.093358|        00000000|           -|                null|    null|null|70569|\n",
      "| 2343|   Q|de:08111:2609:0:3|  de:08111:2609|       Rtg Dachswald|48.746838| 9.120748|        00000000|           -|                null|    null|null|70569|\n",
      "| 2439|   Q| de:08111:264:0:3| de:08111:264:3|Ersatzhaltestelle...| 48.79967| 9.234627|        00000000|           -|                null|    null|null|70372|\n",
      "| 2767|   A|    de:08111:33:1|    de:08111:33|     St Uff-Kirchhof| 48.80593| 9.230854|        00000000|           -|                null|    null|null|70372|\n",
      "| 3840|   A|  de:08111:6115:8|  de:08111:6115| Bahnsteig Gl. 15+16| 48.78464| 9.183648|        00000000|           -|                null|    null|null|70173|\n",
      "| 3874|   Q|de:08111:6116:1:1|de:08111:6116:2|                   1|48.790688| 9.181151|        00000000|           -|                null|    null|null|70191|\n",
      "| 4497|   A|  de:08111:6291:1|  de:08111:6291|       St Pragsattel|48.833908| 9.195506|        00000000|           -|                null|    null|null|70437|\n",
      "| 4847|   A|   de:08111:78:91|    de:08111:78|                  91|48.784935| 9.211352|        00000000|           -|                null|    null|null|70188|\n",
      "| 5269|   Q|de:08115:2363:0:4|  de:08115:2363|                   4| 48.80618| 9.011989|        00000000|           -|                null|    null|null|71229|\n",
      "| 6033|   Q|de:08115:4588:1:3|de:08115:4588:1|                   3|48.768475| 8.852008|        00000000|           -|                null|    null|null|71263|\n",
      "+-----+----+-----------------+---------------+--------------------+---------+---------+----------------+------------+--------------------+--------+----+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-----+----+-----------------+---------------+--------------------+---------+---------+----------------+------------+--------------------+--------+-----+-----------------+\n",
      "|SeqNo|Type|             DHID|         Parent|                Name| Latitude|Longitude|MunicipalityCode|Municipality|MunicipalityCode_ZIP|name_ZIP|  PLZ|ZIP_Group_Station|\n",
      "+-----+----+-----------------+---------------+--------------------+---------+---------+----------------+------------+--------------------+--------+-----+-----------------+\n",
      "|  138|   A|  de:07334:1747:2|  de:07334:1747|              >Wörth|49.208244| 8.373125|        00000000|           -|                null|    null|76726|              767|\n",
      "|  370|   A| de:07334:32490:2| de:07334:32490|                 Bus| 49.07942| 8.198232|        00000000|           -|                null|    null|76870|              768|\n",
      "|  971|   A|  de:08111:136:90|   de:08111:136|   Zug. Kirchh. Str.|48.738213| 9.233127|        00000000|           -|                null|    null|70619|              706|\n",
      "| 1016|   A|  de:08111:145:80|   de:08111:145|   Bahnüb. Giebelstr|48.802643| 9.092442|        00000000|           -|                null|    null|70499|              704|\n",
      "| 1042|   A|   de:08111:147:2|   de:08111:147|                NBus| 48.80714| 9.105045|        00000000|           -|                null|    null|70499|              704|\n",
      "| 1143|   A|   de:08111:154:2|   de:08111:154|    St Pfostenwäldle|48.810143| 9.149934|        00000000|           -|                null|    null|70469|              704|\n",
      "| 1212|   A|  de:08111:163:90|   de:08111:163|Zug. Neue Weinsteige|48.754753|  9.17435|        00000000|           -|                null|    null|70180|              701|\n",
      "| 1278|   A|  de:08111:2053:1|  de:08111:2053|                   1|48.771847| 9.162825|        00000000|           -|                null|    null|70178|              701|\n",
      "| 1690|   Q|de:08111:2380:0:4|  de:08111:2380|                   4| 48.81881| 9.242263|        00000000|           -|                null|    null|70374|              703|\n",
      "| 2000|   Q|de:08111:2506:0:3|  de:08111:2506|                   3| 48.79206| 9.260705|        00000000|           -|                null|    null|70327|              703|\n",
      "| 2264|   Q|de:08111:2591:0:3|  de:08111:2591|Lauchhau Endhalte...|48.738968| 9.093358|        00000000|           -|                null|    null|70569|              705|\n",
      "| 2343|   Q|de:08111:2609:0:3|  de:08111:2609|       Rtg Dachswald|48.746838| 9.120748|        00000000|           -|                null|    null|70569|              705|\n",
      "| 2439|   Q| de:08111:264:0:3| de:08111:264:3|Ersatzhaltestelle...| 48.79967| 9.234627|        00000000|           -|                null|    null|70372|              703|\n",
      "| 2767|   A|    de:08111:33:1|    de:08111:33|     St Uff-Kirchhof| 48.80593| 9.230854|        00000000|           -|                null|    null|70372|              703|\n",
      "| 3840|   A|  de:08111:6115:8|  de:08111:6115| Bahnsteig Gl. 15+16| 48.78464| 9.183648|        00000000|           -|                null|    null|70173|              701|\n",
      "| 3874|   Q|de:08111:6116:1:1|de:08111:6116:2|                   1|48.790688| 9.181151|        00000000|           -|                null|    null|70191|              701|\n",
      "| 4497|   A|  de:08111:6291:1|  de:08111:6291|       St Pragsattel|48.833908| 9.195506|        00000000|           -|                null|    null|70437|              704|\n",
      "| 4847|   A|   de:08111:78:91|    de:08111:78|                  91|48.784935| 9.211352|        00000000|           -|                null|    null|70188|              701|\n",
      "| 5269|   Q|de:08115:2363:0:4|  de:08115:2363|                   4| 48.80618| 9.011989|        00000000|           -|                null|    null|71229|              712|\n",
      "| 6033|   Q|de:08115:4588:1:3|de:08115:4588:1|                   3|48.768475| 8.852008|        00000000|           -|                null|    null|71263|              712|\n",
      "+-----+----+-----------------+---------------+--------------------+---------+---------+----------------+------------+--------------------+--------+-----+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+-----------------+---------------+--------------------+---------+---------+----------------+------------+--------------------+--------+-----+-----------------+\n",
      "|SeqNo|Type|             DHID|         Parent|                Name| Latitude|Longitude|MunicipalityCode|Municipality|MunicipalityCode_ZIP|name_ZIP|  PLZ|ZIP_Group_Station|\n",
      "+-----+----+-----------------+---------------+--------------------+---------+---------+----------------+------------+--------------------+--------+-----+-----------------+\n",
      "|  138|   A|  de:07334:1747:2|  de:07334:1747|              >Wörth|49.208244| 8.373125|        00000000|           -|                null|    null|76726|              767|\n",
      "|  370|   A| de:07334:32490:2| de:07334:32490|                 Bus| 49.07942| 8.198232|        00000000|           -|                null|    null|76870|              768|\n",
      "|  971|   A|  de:08111:136:90|   de:08111:136|   Zug. Kirchh. Str.|48.738213| 9.233127|        00000000|           -|                null|    null|70619|              706|\n",
      "| 1016|   A|  de:08111:145:80|   de:08111:145|   Bahnüb. Giebelstr|48.802643| 9.092442|        00000000|           -|                null|    null|70499|              704|\n",
      "| 1042|   A|   de:08111:147:2|   de:08111:147|                NBus| 48.80714| 9.105045|        00000000|           -|                null|    null|70499|              704|\n",
      "| 1143|   A|   de:08111:154:2|   de:08111:154|    St Pfostenwäldle|48.810143| 9.149934|        00000000|           -|                null|    null|70469|              704|\n",
      "| 1212|   A|  de:08111:163:90|   de:08111:163|Zug. Neue Weinsteige|48.754753|  9.17435|        00000000|           -|                null|    null|70180|              701|\n",
      "| 1278|   A|  de:08111:2053:1|  de:08111:2053|                   1|48.771847| 9.162825|        00000000|           -|                null|    null|70178|              701|\n",
      "| 1690|   Q|de:08111:2380:0:4|  de:08111:2380|                   4| 48.81881| 9.242263|        00000000|           -|                null|    null|70374|              703|\n",
      "| 2000|   Q|de:08111:2506:0:3|  de:08111:2506|                   3| 48.79206| 9.260705|        00000000|           -|                null|    null|70327|              703|\n",
      "| 2264|   Q|de:08111:2591:0:3|  de:08111:2591|Lauchhau Endhalte...|48.738968| 9.093358|        00000000|           -|                null|    null|70569|              705|\n",
      "| 2343|   Q|de:08111:2609:0:3|  de:08111:2609|       Rtg Dachswald|48.746838| 9.120748|        00000000|           -|                null|    null|70569|              705|\n",
      "| 2439|   Q| de:08111:264:0:3| de:08111:264:3|Ersatzhaltestelle...| 48.79967| 9.234627|        00000000|           -|                null|    null|70372|              703|\n",
      "| 2767|   A|    de:08111:33:1|    de:08111:33|     St Uff-Kirchhof| 48.80593| 9.230854|        00000000|           -|                null|    null|70372|              703|\n",
      "| 3840|   A|  de:08111:6115:8|  de:08111:6115| Bahnsteig Gl. 15+16| 48.78464| 9.183648|        00000000|           -|                null|    null|70173|              701|\n",
      "| 3874|   Q|de:08111:6116:1:1|de:08111:6116:2|                   1|48.790688| 9.181151|        00000000|           -|                null|    null|70191|              701|\n",
      "| 4497|   A|  de:08111:6291:1|  de:08111:6291|       St Pragsattel|48.833908| 9.195506|        00000000|           -|                null|    null|70437|              704|\n",
      "| 4847|   A|   de:08111:78:91|    de:08111:78|                  91|48.784935| 9.211352|        00000000|           -|                null|    null|70188|              701|\n",
      "| 5269|   Q|de:08115:2363:0:4|  de:08115:2363|                   4| 48.80618| 9.011989|        00000000|           -|                null|    null|71229|              712|\n",
      "| 6033|   Q|de:08115:4588:1:3|de:08115:4588:1|                   3|48.768475| 8.852008|        00000000|           -|                null|    null|71263|              712|\n",
      "+-----+----+-----------------+---------------+--------------------+---------+---------+----------------+------------+--------------------+--------+-----+-----------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "[{helper.py:368} INFO - Repartitioning dataframe that has 200 partition(s)\n",
      "[{helper.py:370} INFO - Successfully repartitioned dataframe to 1 partition\n",
      "[{helper.py:371} INFO - Writing table_stations_with_zip_final to s3a://4a4e3668e3/data_mart/table_stations_with_zip_final.parquet ...\n",
      "[{helper.py:385} ERROR - Error writing table_stations_with_zip_final to parquet. Reason:\n",
      "An error occurred while calling o718.parquet.\n",
      ": org.apache.spark.SparkException: Job aborted.\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:198)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:159)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\n",
      "\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n",
      "\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:676)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:285)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:271)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:229)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:566)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 8 in stage 110.0 failed 1 times, most recent failure: Lost task 8.0 in stage 110.0 (TID 5732, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"C:\\Spark\\spark-2.4.3-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 362, in main\n",
      "  File \"C:\\Spark\\spark-2.4.3-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 715, in read_int\n",
      "    length = stream.read(4)\n",
      "  File \"C:\\Python\\Python37\\lib\\socket.py\", line 589, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "socket.timeout: timed out\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage8.agg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage8.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n",
      "\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n",
      "\tat scala.Option.foreach(Option.scala:257)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:167)\n",
      "\t... 33 more\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"C:\\Spark\\spark-2.4.3-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 362, in main\n",
      "  File \"C:\\Spark\\spark-2.4.3-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 715, in read_int\n",
      "    length = stream.read(4)\n",
      "  File \"C:\\Python\\Python37\\lib\\socket.py\", line 589, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "socket.timeout: timed out\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage8.agg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage8.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{helper.py:415} ERROR - Error creating dataframe table_stations_with_zip_final.parquet. Reason: 'Unable to infer schema for Parquet. It must be specified manually.;'\n",
      "[{helper.py:413} INFO - Successfully imported table_rental_location.parquet to Spark dataframe. Sourced object imported.\n",
      "+----------------+\n",
      "|ZIP_Group_Rental|\n",
      "+----------------+\n",
      "|             296|\n",
      "|             691|\n",
      "|             675|\n",
      "|             467|\n",
      "|             451|\n",
      "|             944|\n",
      "|             853|\n",
      "|             125|\n",
      "|             666|\n",
      "|             926|\n",
      "|             124|\n",
      "|             447|\n",
      "|             591|\n",
      "|             613|\n",
      "|             475|\n",
      "|             574|\n",
      "|             740|\n",
      "|             030|\n",
      "|             205|\n",
      "|             581|\n",
      "+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'ZIP_Group_Station'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12868/865629464.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;31m# GROUP Station data by groups of zip codes. These groups match the rental data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mprocess_outbound\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroup_stations_by_zipcode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspark\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Python\\_Working\\DatEng_Capstone\\etl\\process_outbound.py\u001b[0m in \u001b[0;36mgroup_stations_by_zipcode\u001b[1;34m(spark, input_path, output_path)\u001b[0m\n\u001b[0;32m    442\u001b[0m     \u001b[1;31m# JOIN Rental and Station tables on zip-group\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    443\u001b[0m     \u001b[1;31m# left join keeps the zip group of the rentals and adds all stations with matching zip group\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 444\u001b[1;33m     \u001b[0mcond\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtable_rental_location\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mZIP_Group_Rental\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mtable_stations_with_zip\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mZIP_Group_Station\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    445\u001b[0m     \u001b[0mtable_stations_grouped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtable_rental_location\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    446\u001b[0m         \u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtable_stations_with_zip\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcond\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"left\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'ZIP_Group_Station'"
     ]
    }
   ],
   "source": [
    "# GET additional data for stations\n",
    "if \"process_stations\" in work_mode:\n",
    "\n",
    "    # ADD zip code to Station data\n",
    "    process_outbound.add_zipcode_to_stations(spark, input_path=output_path, output_path=output_path)\n",
    "\n",
    "    # GROUP Station data by groups of zip codes. These groups match the rental data\n",
    "    process_outbound.group_stations_by_zipcode(spark, input_path=output_path, output_path=output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "37388b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{1377736684.py:1} INFO - Transformed data and saved to data lake\n",
      "\n"
     ]
    }
   ],
   "source": [
    "helper.logger.info(\"Transformed data and saved to data lake\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae8a2e4",
   "metadata": {},
   "source": [
    "## UTILIZE data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "db36a13c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{helper.py:413} INFO - Successfully imported table_stations_grouped.parquet to Spark dataframe. Sourced object imported.\n",
      "[{helper.py:413} INFO - Successfully imported table_rental_location_partA.parquet to Spark dataframe. Sourced object imported.\n",
      "[{helper.py:413} INFO - Successfully imported table_rental_location_partB.parquet to Spark dataframe. Sourced object imported.\n"
     ]
    }
   ],
   "source": [
    "# PRELOAD data\n",
    "table_name = \"table_stations_grouped\"\n",
    "table_stations_grouped = helper.create_df_from_parquet(spark, table_name, output_path).cache()\n",
    "\n",
    "table_rental_location_coords = process_outbound.load_table_rental_locations_with_coordindates(spark, output_path).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "732a0a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SELECT scoutid that represents an apartment\n",
    "scoutId = 112215775 # 106981998 # 109001404 # 115209583 #109001404 #109001404 #115209583 #109001404# 115209583 # 109001404  #115646074 #115209583 #106981998  #115646074  #112215775  #109001404  #115209583\n",
    "\n",
    "# 115209583 is a rental offer in Leipzig, Nordstr."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b9e2737d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{process_outbound.py:511} INFO - Found valid scoutId 112215775. Querying nearest stations...\n",
      "\n",
      "Rental location:\n",
      "(51.2800541, 12.3605674, 'Koburger Straße 77, 04416, Leipzig, Sachsen')\n",
      "\n",
      "Nearest Stations:\n",
      "\n",
      "      Latitude  Longitude          Parent            DHID  \\\n",
      "235  51.278854  12.359271  de:14729:12988  de:14729:12988   \n",
      "47   51.277004  12.362433  de:14729:10996  de:14729:10996   \n",
      "60   51.283718  12.364071  de:14729:12037  de:14729:12037   \n",
      "26   51.280087  12.354587  de:14729:13289  de:14729:13289   \n",
      "116  51.285164  12.367231  de:14729:12030  de:14729:12030   \n",
      "115  51.277477  12.368747  de:14729:10999  de:14729:10999   \n",
      "202  51.272461  12.356539  de:14729:12986  de:14729:12986   \n",
      "107  51.270969  12.360272  de:14729:12767  de:14729:12767   \n",
      "99   51.274311  12.367885   de:14729:3854   de:14729:3854   \n",
      "87   51.279148  12.370060  de:14729:10998  de:14729:10998   \n",
      "\n",
      "                                   Name  \n",
      "235      Markkleeberg, Gautzscher Platz  \n",
      "47                   Markkleeberg, West  \n",
      "60          Markkleeberg, Sonnesiedlung  \n",
      "26            Markkleeberg, Mehringstr.  \n",
      "116           Markkleeberg, Energiestr.  \n",
      "115                  Markkleeberg, Ring  \n",
      "202          Markkleeberg, Am Eulenberg  \n",
      "107             Markkleeberg, Sonnenweg  \n",
      "99   Markkleeberg, West, Wasserturmstr.  \n",
      "87              Markkleeberg, S-Bahnhof  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# GET nearest stations for a given scoutId\n",
    "scout_apartment_infos, nearest_stations = process_outbound.query_nearest_stations(\n",
    "        spark,\n",
    "        input_path=output_path,\n",
    "        scoutId=scoutId,\n",
    "        table_rental_location_coords=table_rental_location_coords,\n",
    "        table_stations_grouped=table_stations_grouped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "51daf57a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<folium.folium.Map at 0x1576a479548>,\n",
       " 'file://C:\\\\Python\\\\_Working\\\\DatEng_Capstone\\\\temp.html')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_outbound.show_nearest_station(spark, output_path, scoutId, scout_apartment_infos, nearest_stations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c7fc65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
